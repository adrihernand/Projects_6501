---
title: "Homework 2"
output: html_document
---

#### Question 3.1

Using the same data set (credit_card_data.txt or credit_card_data-headers.txt) as in Question 2.2, use the ksvm or kknn function to find a good classifier:
(a) using cross-validation (do this for the k-nearest-neighbors model; SVM is optional); and
(b) splitting the data into training, validation, and test data sets (pick either KNN or SVM; the other is optional).


## Procedure to find the best classifier for knn
### a) Using cross-validation

As introduction, my knn model with all the data had an accuracy of 85,32% when k=12. This is the one that was my best classifier for knn in the previous assignment. 

For this assignment, I loaded the data and displayed the first rows. Then I proceeded to set the seed to obtain the same results all the time. 

In order to perform the cross validation of knn, I followed the "leave one out" approach using the train.kknn option. This method is the same as saying that all the data is going to be divided in the smallest possible chunks, and that is, that each observation will become a chunk. Since I have 654 observations, this process is going to train the model doing 654 iterations and each time is going to use 653 observations and will leave 1(one) observation out. In addition, it will determine the best k of the knn model from k=1 to k=100.

After the model was trained, the prediction follows with the data specified and making use of the best k that was determined previously.

Finally, I used confusion matrix to investigate the accuracy of the model using cross-validation. Here is the code:

```{r}

# Loading data and seeing it in a table format
route <-"C:\\Users\\adri_\\Documents\\Gatech\\ISYE6501\\week 2_validation_clustering\\Fall2020hw2\\data 3.1\\credit_card_data-headers.txt"
credit_data <-read.table(route,sep="\t", header=TRUE)
head(credit_data)


# Set the seed to ensure that I get the same result all the time
set.seed(1)


# Cross validation of knn using the leave one out method
library(kknn)
cross_val_knn <- train.kknn(R1~., data = credit_data, kmax = 100,  scale=TRUE)
summary(cross_val_knn)

preds <- round(fitted(cross_val_knn)[[58]][1:nrow(credit_data)], digits = 0)

library(caret)
confusionMatrix(factor(credit_data[,11],levels = c(1,0)) ,factor(preds, levels = c(1,0)))

```
#### Conclusion of part(a):
The best number of k is when k = 58 and the accuracy using cross-validation on knn is 83.94 %. These results are expected as they indicate that when using less data to train the model we can have better accuracy when the model is applied to new data.It will generalize better and we will avoid overfitting.

### b)Using the method of splitting the data into training, validation, and test data sets
First, I cleaned the environment, loaded the data and set the seed to obtain the same result all the time.

I divided the data as follows: 70% for training purposes, 15% for validation purposes and 15% for testing purposes. 

The data of each part was picked randomly of all the observations. After this, I proceeded to create 2 (two) functions, one function that runs the process of training the model and the other function to run the process of calculating the accuracy of the model trained given a number of k. I was curious to see the accuracy only when k=12, k=15 and k= 58 for this homework as these were values of k's that I had previously analyzed using all the data and cross-validation. Here is the code:


```{r}
# Splitting data method for knn. Decision is to use 70% as training set, 15% as validation set and 15% as test set. 

rm(list = ls())

# Load data and see it in a table format
route <-"C:\\Users\\adri_\\Documents\\Gatech\\ISYE6501\\week 2_validation_clustering\\Fall2020hw2\\data 3.1\\credit_card_data-headers.txt"
credit_data <-read.table(route,sep="\t", header=TRUE)
head(credit_data)


# Set the seed to ensure that I get the same result all the time
set.seed(1)


# Randomly taking 70% to use for training 
sampling_70 = sample(nrow(credit_data), round(nrow(credit_data)*.7))
training_set = credit_data[sampling_70, ]

# Split the remaining 30% into validation and test
remaining_30 = credit_data[-sampling_70, ]
half_of_30 = sample(nrow(remaining_30), round(nrow(remaining_30)*.5))


validation_set = remaining_30[half_of_30, ]
test_set = remaining_30[-half_of_30, ]


### Create a function that trains a model with neighbors 
train.model = function(neighbors)
{
  model = kknn(R1 ~., test_set, validation_set, k = neighbors, scale = TRUE)
  
  # store the predictions
  model_fitted = as.matrix(model$fitted.values)
  
  # round the fitted values
  model_fitted_rounded = as.matrix(lapply(model_fitted[, 1], round))
  
  # merge rounded that values back into model_fitted
  model_results = data.frame(validation_set[, 'R1'], model_fitted_rounded)
  
  colnames(model_results) = c("Actual", "Predicted")
  return(model_results)
}


### Create a function that calculates the accuracy based on predictions
eval.model = function(model)
{
  results <- vector("list", nrow(model))  
  for(i in 1:nrow(model)){
    results[[i]] <- as.integer(model[i, 'Actual'] == model[i, 'Predicted'])
  }
  # calculates the percent of correct predictions
  correct = sum(data.frame(results))
  model_accuracy = correct / nrow(model)
  return(model_accuracy)
}


### Train three models using k values of 12, 15, and 58
model_k12 = train.model(12)
model_k15 = train.model(15)
model_k58 = train.model(58)

### Return accuracy for each model
eval.model(model_k12)
eval.model(model_k15)
eval.model(model_k58)

```
#### Conclusion of part(b):
The best accuracy is when k = 58 which using the split, validation and test sets gives an accuracy of 84.69%. These results are expected as they indicate that when using less data to train the model we can have better accuracy when the model is applied to new data.It will generalize better and we will avoid overfitting.


### Final thoughts on part(a) and part(b) using knn
The best classifier of the scenarios that I analyzed is when k=58 which is 84.69% accurate when applied to new data.This was possible by splitting the data in 70% for training purposes and 30% for validation and testing. 


#### Question 4.1 

Describe a situation or problem from your job, everyday life, current events, etc., for which a clustering model would be appropriate. List some (up to 5) predictors that you might use.


### Analysis
In the oil and gas industry, there's a lot of uncertainty when it comes to determine the facies. Facies are bodies of rocks with specific characteristics such as permeability, porosity and water saturation among others, and these bodies change from one point to another. 

Before drilling wells, geologists are in charge of determining the facies, which in this case are located thousands feet underground, in the area of interest by the field operator company.Geologists typically have to infere petrophysical properties using traditional methods to draw these subsurface maps. Later, these maps are used by other professionals such as reservoir engineers in order to quantify potential hydrocarbon reserves. These maps are not updated regularly despiteof sucessfull completion of wells.


I see clustering as a great technique to help improve the mechanism to understand the facies because clustering is an unsupervised machine learning algorithm that allows us to group based on similarities and without explicitly knowing if something belongs to a specific class. 

I would clustering with predictors such as: permeability, porosity, water saturation, depth and grain size.



#### Question 4.2 

The iris data set iris.txt contains 150 data points, each with four predictor variables and one categorical response. The predictors are the width and length of the sepal and petal of flowers and the response is the type of flower. The data is available from the R library datasets and can be accessed with iris once the library is loaded. It is also available at the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Iris ). The response values are only given to see how well a specific method performed and should not be used to build the model.

Use the R function kmeans to cluster the points as well as possible. Report the best combination of predictors, your suggested value of k, and how well your best clustering predicts flower type.


## Procedure to find the best classifier for clustering
First I loaded the data, check the first rows and columns with the head option and set the seed to make the results always the same. Then I proceeded to understand the data more by checking how many species I have, this procedure was done just for curiosity as we usually don't know this information. I decided to scale the data to avoid having different "weights" on the data, for this part I applied the procedure of min-max normalization.

```{r}

iris_data <- read.table("C:\\Users\\adri_\\Documents\\Gatech\\ISYE6501\\week 2_validation_clustering\\Fall2020hw2\\data 4.2\\iris.txt", header = TRUE)

# checking first rows
head(iris_data)

set.seed(1)


# How many we have from each species
table(iris_data[,5], iris_data$Species)


# scale the data using min max normalization:
iris_data_scaled = iris_data 
for (i in 1:4) { iris_data_scaled[,i] <- (iris_data[,i]-min(iris_data[,i]))/(max(iris_data[,i])-min(iris_data[,i])) }

```
With the data scaled, I decided to investigate 7 possible scenarios of combinations of predictors and each when k=3, k=4, k=5 and k=6. The scenarios are as follows:

a. Assumption: All predictors are taken into account
 + Scenario 1: Using Sepal Length,Sepal Width, Petal Length, Petal Width when k=3, k=4, k=5, and k=6
b. Assumption: three(3) predictors are taken into account
 + Scenario 2: Using Sepal Length, Sepal Width, Petal Length when k=3, k=4, k=5, and k=6
 + Scenario 3: Using Sepal Length, Sepal Width, Petal Width when k=3, k=4, k=5, and k=6
 + Scenario 4: Using Sepal Length, Petal Length, Petal Width when k=3, k=4, k=5, and k=6
 + Scenario 5: Using Sepal Width, Petal Length, Petal Width when k=3, k=4, k=5, and k=6
c. Assumption: two(2) predictors are taken into account
 + Scenario 6: Using Sepal Length, Sepal Width when k=3, k=4, k=5, and k=6
 + Scenario 7: Using Petal Length, Petal Width when k=3, k=4, k=5, and k=6


To make the analysis easier, I created tables of results to identify how many misclassifications occurred in each bucket (k) for each case. 

Here is the code:


```{r}
# Possible scenarios


# 1) Assuming all predictors(there are 4 predictors variables) when k= 3, k=4, k=5, k=6

# Scenario 1: Using sepal length,sepal width, petal length, petal width
cluster_k3 = kmeans(iris_data_scaled[,1:4], 3, iter.max = 10, nstart = 25)
cluster_k4 = kmeans(iris_data_scaled[,1:4], 4, iter.max = 10, nstart = 25)
cluster_k5 = kmeans(iris_data_scaled[,1:4], 5, iter.max = 10, nstart = 25)
cluster_k6 = kmeans(iris_data_scaled[,1:4], 6, iter.max = 10, nstart = 25)


# Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


# 2) Assuming 3 predictors when k= 3, k=4, k=5, k=6

# Scenario 2: Using Sepal Length, Sepal Width, Petal length
cluster_k3 = kmeans(iris_data_scaled[,1:3], 3, iter.max = 10, nstart = 25)
cluster_k4 = kmeans(iris_data_scaled[,1:3], 4, iter.max = 10, nstart = 25)
cluster_k5 = kmeans(iris_data_scaled[,1:3], 5, iter.max = 10, nstart = 25)
cluster_k6 = kmeans(iris_data_scaled[,1:3], 6, iter.max = 10, nstart = 25)

#Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


# Scenario 3: Using Sepal Length, Sepal width, Petal width
cluster_k3 = kmeans(iris_data_scaled[,c(4,1:2)], 3, iter.max = 10, nstart = 25)
cluster_k4 = kmeans(iris_data_scaled[,c(4,1:2)], 4, iter.max = 10, nstart = 25)
cluster_k5 = kmeans(iris_data_scaled[,c(4,1:2)], 5, iter.max = 10, nstart = 25)
cluster_k6 = kmeans(iris_data_scaled[,c(4,1:2)], 6, iter.max = 10, nstart = 25)

#Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


# Scenario 4: Using Sepal Length, Petal length,Petal width
cluster_k3 = kmeans(iris_data_scaled[,c(1,3:4)], 3, iter.max = 10, nstart = 25)
cluster_k4 = kmeans(iris_data_scaled[,c(1,3:4)], 4, iter.max = 10, nstart = 25)
cluster_k5 = kmeans(iris_data_scaled[,c(1,3:4)], 5, iter.max = 10, nstart = 25)
cluster_k6 = kmeans(iris_data_scaled[,c(1,3:4)], 6, iter.max = 10, nstart = 25)

#Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


# Scenario 5: Using Sepal Width,Petal length, Petal width
cluster_k3 = kmeans(iris_data_scaled[,2:4], 3, iter.max = 10, nstart = 20)
cluster_k4 = kmeans(iris_data_scaled[,2:4], 4, iter.max = 10, nstart = 20)
cluster_k5 = kmeans(iris_data_scaled[,2:4], 5, iter.max = 10, nstart = 20)
cluster_k6 = kmeans(iris_data_scaled[,2:4], 6, iter.max = 10, nstart = 20)

#Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


# 3) Assuming 2 predictors when k= 3, k=4, k=5, k=6

# Scenario 6: Using Sepal Length, Sepal  Width
cluster_k3 = kmeans(iris_data_scaled[,1:2], 3, iter.max = 10, nstart = 20)
cluster_k4 = kmeans(iris_data_scaled[,1:2], 4, iter.max = 10, nstart = 20)
cluster_k5 = kmeans(iris_data_scaled[,1:2], 5, iter.max = 10, nstart = 20)
cluster_k6 = kmeans(iris_data_scaled[,1:2], 6, iter.max = 10, nstart = 20)

#Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


# Scenario 7: Using Petal Length, Petal Width
cluster_k3 = kmeans(iris_data_scaled[,3:4], 3, iter.max = 10, nstart = 20)
cluster_k4 = kmeans(iris_data_scaled[,3:4], 4, iter.max = 10, nstart = 20)
cluster_5 = kmeans(iris_data_scaled[,3:4], 5, iter.max = 10, nstart = 20)
cluster_k6 = kmeans(iris_data_scaled[,3:4], 6, iter.max = 10, nstart = 20)

#Comparing clusters with the species.
table(cluster_k3$cluster, iris_data$Species)
table(cluster_k4$cluster, iris_data$Species)
table(cluster_k5$cluster, iris_data$Species)
table(cluster_k6$cluster, iris_data$Species)


```
### Final thoughts
The best classifier is Scenario 5, in which I used as predictors Sepal Width, Petal Length and Petal Width and when k = 6 because in the first bucket(k) it captured perfectly "virginica", in the second bucket(k) captured perfectly "versicolor",in the third bucket(k) captured perfectly "setosa", in the forth bucket(k) it only misclassified 1 datapoint, in the fifth bucket(k) it misclassified 4 datapoints and in the last bucket(k) it perfectly captured "setosa". This means, this run only had 5 datapoints that can be interpreted as "errors" out of the 150 datapoints, assuming six(6) clusters for these specific predictors.My conclusion is that it classified very good!
